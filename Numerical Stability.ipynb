{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "In this section we introduce methods of measuring precision - number of significant digits, absolute and relative error. We also encourage the reader to use absolute error in order to be most precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significant Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of significant digits may be imprecise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definition 1: Round down to p-sig. digit number\n",
    "x  = 0.90\n",
    "x1 = 0.99 # 2 correct significant digit, actual difference 0.09\n",
    "x2 = 0.89 # 1 correct significant digit, actual difference 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definition 2: Round to nearest p-sig. digit number\n",
    "y  = 0.9951 # --> 0.10\n",
    "y1 = 0.9499 # --> 0.90 , only 1 correct sig. digit\n",
    "y2 = 1.0000 # --> 0.10 , 3 correct sig. digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last definition avoids issues that the previous two had, but it's still non-ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definition 3: Numbers x and x' match to p-sig. digits if x - x' < half a unit in p-th sig. digit of x\n",
    "x1 = 0.123\n",
    "x2 = 0.127\n",
    "# 0.004 < (0.01 / 2) => x1 and x2 match in 2 significant digits according to this definition wchich may be slightly confusing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute and relative error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute error: 0.002000000\n",
      "Relative error: 0.000182632\n"
     ]
    }
   ],
   "source": [
    "def absolute_error(true_value, approx_value):\n",
    "    return abs(true_value - approx_value)\n",
    "\n",
    "print 'Absolute error: {0:.9f}'.format(absolute_error(10.951, 10.949))\n",
    "\n",
    "def relative_error(true_value, approx_value):\n",
    "    return absolute_error(true_value, approx_value) / abs(true_value)\n",
    "\n",
    "print 'Relative error: {0:.9f}'.format(relative_error(10.951, 10.949))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Error for Non-Scalars\n",
    "For non-scalars, calculating normalized value *||x|| = max / sum*, implies smaller components of **x** are bound by absolute error only. Consider compensative relative error: *max(i) |xi - xi’| / |xi|*, which puts all components on equal footing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relative_error(true_value, approx_value):\n",
    "    return np.max(np.fabs((true_value - approx_value) / true_value))\n",
    "\n",
    "x_value = np.array([10000, 0.01])\n",
    "x_approx = np.array([9999, 0.02])\n",
    "                   \n",
    "print relative_error(x_value, x_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources of Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncation error (discretization error)\n",
    "Error coming from representing a function or continuous variable using finite number of evaluations - outside of scope of this notebook, mentioned for completeness only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round-off error\n",
    "* Difference between calculated approximation and exact value due to rounding\n",
    "* Related to representation error, which is due to representing numbers with finite number of digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32- vs 64-bit representation difference: 1.49011611383e-09\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.float64(0.1)\n",
    "y = np.float32(0.1)\n",
    "print '32- vs 64-bit representation difference:', abs(x - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rounding Multiple Times Can Accumulate Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# For explanation why we we use np.round rather than default Python 2.7.3 round function, see below\n",
    "x = 9.945309 \n",
    "print np.round(x, 2), np.round(np.round(x, 2), 1)\n",
    "print np.round(x, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Differences between round and np.round\n",
    "\n",
    "There are few differences between built-in Python 2.7 round function and numpy (a)round:\n",
    "\n",
    "* The built in function rounds away from zero\n",
    "* Numpy round rounds to even, which tends to skew the results less and is a commonly accepted rounding method\n",
    "* From my (limited) exerience it looks numpy round is much better behaved in dealing with decimal-to-binary float rounding errors\n",
    "\n",
    "Note that Python 3 has a different round function that behaves more similarly to numpy round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t -3.0\t -3.0\t -3.0\n",
      "\t -2.5\t -3.0\t -2.0\n",
      "\t -2.0\t -2.0\t -2.0\n",
      "\t -1.5\t -2.0\t -2.0\n",
      "\t -1.0\t -1.0\t -1.0\n",
      "\t -0.5\t -1.0\t  0.0\n",
      "\t  0.0\t  0.0\t  0.0\n",
      "\t  0.5\t  1.0\t  0.0\n",
      "\t  1.0\t  1.0\t  1.0\n",
      "\t  1.5\t  2.0\t  2.0\n",
      "\t  2.0\t  2.0\t  2.0\n",
      "\t  2.5\t  3.0\t  2.0\n",
      "\t  3.0\t  3.0\t  3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(13):\n",
    "    x = -3 + 0.5 * i\n",
    "    print '\\t{0:5.1f}\\t{1:5.1f}\\t{2:5.1f}'.format(x, round(x), np.round(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss of significance\n",
    "Error in floating point arithmetic when an operation increases relative error substantially more than absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual d value: Approx d value: 0.0001\n",
      "Absolute error: 0.000023000\n",
      "Relative error: 0.186991870\n"
     ]
    }
   ],
   "source": [
    "x_value = 0.123123\n",
    "y_value = 0.123000\n",
    "# We want to learn the value of d = x - y:\n",
    "d_value = x_value - y_value\n",
    "print 'Actual d value:', \n",
    "# Assuming we're apprximating above calculation with a 4 decimal digits precision:\n",
    "import numpy as np\n",
    "d_approx = np.round(x_value, 4) - np.round(y_value, 4)\n",
    "print 'Approx d value:', d_approx\n",
    "print 'Absolute error: {0:.9f}'.format(abs(d_value - d_approx))\n",
    "print 'Relative error: {0:.9f}'.format(abs((d_value - d_approx) / d_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision vs Accuracy\n",
    "* Precision is the accuracy of basic arithmetic operations used in the computation\n",
    "* Accuracy is the absolute or relative error of the approximate quantity\n",
    "* NOTE: Accuracy is not limited by precision, finite precision arithmetic can simulate any precision with more computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward and Forward Errors\n",
    "* Forward error - error of the computed value\n",
    "* Backward error:\n",
    "  * Let y = f(x), given x we approximate f(x) with y’\n",
    "  * Let dx be the smallest quantity where y’ = f(x + dx) in exact computation\n",
    "  * Then dx is the backward error\n",
    "* Benefits of using backward error:\n",
    "  * Unifies error w/ perturbation in the data\n",
    "  * Removes the need to calculate forward error\n",
    "* Forward-backward error:\n",
    "  * f(x + dx) = y + dy\n",
    "  * Used to define stability of computation where just using backward error isn’t possible, e.g. sin, cos\n",
    "* **If rounding errors are dominant source of errors, we call an algorithm numerically stable if it is stable in forward-backward error sense**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition Number\n",
    "* Condition number of a function with respect to its arguments is used to measure how how much the output of the function will change for a small change in the input\n",
    "* As a rule of thumb, if the condition number kappa(A) = 10^k, then you may lose up to k digits of accuracy on top of what would be lost to the numerical method due to loss of precision from arithmetic methods\n",
    "* A problem with a low condition number is said to be well-conditioned, while a problem with a high condition number is said to be ill-conditioned\n",
    "* For example, the condition number associated with the linear equation Ax = b gives a bound on how inaccurate the solution x will be after approximation. Note that this is before the effects of round-off error are taken into account; conditioning is a property of the matrix, not the algorithm or floating point accuracy of the computer used to solve the corresponding system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Roots of a Polynomial\n",
    "Finding roots of a polynomial is not a well-conditioned problem, let's look at Wilkinson’s polynomial as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xa4b07b8>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEGCAYAAABxfL6kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa4b0b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wilkinson's polynomial is defined as p(x) = (x - 1)(x - 2)...(x - 20)\n",
    "import numpy as np\n",
    "x = np.linspace(0, 20, 4000)\n",
    "y = 1\n",
    "for i in range(1, 20):\n",
    "    y *= (x - i)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.ylim([-4e11, 4e11])\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quoting Wikipedia:\n",
    "* *\"If the coefficient of x19 is decreased from −210 by 2−23 to −210.0000001192, then the polynomial value w(20) decreases from 0 to −6.25×10^17, and the root at x = 20 grows to x ≈ 20.8. The roots at x = 18 and x = 19 collide into a double root at x ≈ 18.62 which turns into a pair of complex conjugate roots at x ≈ 19.5±1.9i as the perturbation increases further.\"*\n",
    "* *\"Wilkinson's polynomial is often used to illustrate the **undesirability of naively computing eigenvalues of a matrix by first calculating the coefficients of the matrix's characteristic polynomial** and then finding its roots, since using the coefficients as an intermediate step may introduce an extreme ill-conditioning even if the original problem was well conditioned\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.float64(0.1)\n",
    "y = np.float32(0.1)\n",
    "print 'Relative error: ', abs(x - y)\n",
    "print 'Absolute error: ', abs(x - y) / abs(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Cancellation examples</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sin, cos\n",
    "\n",
    "near_zero = 1.2e-8\n",
    "\n",
    "def f(x):\n",
    "    return (1 - cos(x)) / x**2\n",
    "print f(near_zero)\n",
    "\n",
    "# Rewrite f(x) as g(x) (meaning f(x) == g(x)):\n",
    "def g(x):\n",
    "    return 0.5 * (2 * sin(x / 2) / x)**2\n",
    "\n",
    "print g(near_zero)\n",
    "\n",
    "# for i in range(1, 101):\n",
    "#     val = i * 0.1\n",
    "#     print f(val) - g(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing numbers: https://docs.python.org/2/library/math.html https://en.m.wikipedia.org/wiki/Kahan_summation_algorithm\n",
    "Interestingly, IPython Notebook appears to do the right thing by default, what?\n",
    "IPython session:\n",
    "\n",
    "In [6]: from math import fsum\n",
    "\n",
    "In [7]: sum([0.1] * 10) Out[7]: 0.9999999999999999\n",
    "\n",
    "In [8]: fsum([0.1] * 10) Out[8]: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import fsum\n",
    "print sum([0.1] * 10)\n",
    "print fsum([0.1] * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Calculations without subtractions are fine, right?</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def naive_e(n):\n",
    "    return (1 + 1.0 / n)**n\n",
    "\n",
    "from math import exp\n",
    "e = exp(1)\n",
    "\n",
    "for i in range(5, 20):\n",
    "    print naive_e(10**i) - e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def identity(x, n):\n",
    "    for i in xrange(n):\n",
    "        x = sqrt(x)\n",
    "    for i in xrange(n):\n",
    "        x = x**2\n",
    "    return x\n",
    "\n",
    "x = 2\n",
    "for i in xrange(35, 60):\n",
    "    print x - identity(x, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import fsum, pi\n",
    "\n",
    "def basel1(n):\n",
    "    r = 0\n",
    "    for i in xrange(n):\n",
    "        r += 1.0 / (i + 1)**2\n",
    "    return r\n",
    "\n",
    "def basel2(n):\n",
    "    r = 0\n",
    "    for i in reversed(xrange(n)):\n",
    "        r += 1.0 / (i + 1)**2\n",
    "    return r\n",
    "\n",
    "def basel3(n):\n",
    "    return sum(1.0 / (i + 1)**2 for i in xrange(n))\n",
    "\n",
    "def basel4(n):\n",
    "    return fsum(1.0 / (i + 1)**2 for i in xrange(n))\n",
    "        \n",
    "x = pi**2 / 6\n",
    "n = 1000000\n",
    "\n",
    "print x - basel1(n)\n",
    "print x - basel2(n)\n",
    "print x - basel3(n)\n",
    "print x - basel4(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h1>Sometimes the rounding errors cancel out and produce a result more accurate than the intermediate calculations</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Computing f(x) = (exp(x) - 1) / x == sum(x^i / (i + 1)!)\n",
    "\n",
    "from math import exp, log\n",
    "\n",
    "def f1(x):\n",
    "    if 0 == x:\n",
    "        return 1\n",
    "    return (exp(x) - 1) / x\n",
    "\n",
    "def f2(x):\n",
    "    if 0 == x:\n",
    "        return 1\n",
    "    y = exp(x)\n",
    "    return (y - 1) / log(y)\n",
    "\n",
    "# f(epsilon) ~= 1\n",
    "for i in range(8, 15):\n",
    "    print abs(1 - f1(1.0 / (10**i))), abs(1 - f2(1.0 / (10**i)))\n",
    "    \n",
    "# NOTE: Above doesn't hold if we calculate for powers of 2!\n",
    "# for i in range(30, 40):\n",
    "#     print abs(1 - f1(1.0 / (2**i))), abs(1 - f2(1.0 / (2**i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Rounding errors are not random</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def r(x):\n",
    "    # Calculate value of a rational function using Horner's rule (https://en.wikipedia.org/wiki/Horner%27s_method)\n",
    "    # More on the function can be looked up on Wolfram's Alpha:\n",
    "    # http://www.wolframalpha.com/input/?i=f(x)+%3D+(622.0+-+x+*+(751.0+-+x+*+(324.0+-+x+*+(59.0+-+4+*+x))))+%2F+(112+-+x+*+(151+-+x+*+(72+-+x+*+(14+-+x))))&t=crmtb01\n",
    "    p = 622.0 - x * (751.0 - x * (324.0 - x * (59.0 - 4 * x)))\n",
    "    q = 112 - x * (151 - x * (72 - x * (14 - x)))\n",
    "    return p / q\n",
    "\n",
    "def calc_f(a):\n",
    "    t = np.array([a + k * 2**-52 for k in xrange(400)])\n",
    "    t = r(t)\n",
    "    t -= t[0]\n",
    "    t *= 1.0 / max(abs(t))\n",
    "    return t\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot(t):\n",
    "    plt.plot(t, linestyle='--', marker='o')\n",
    "    plt.show()\n",
    "    \n",
    "for a in [1.606, 4, 8, 16, 32]:\n",
    "    plot(calc_f(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Designing stable algorithms</h1>\n",
    "\n",
    "0. **Try to avoid subtracting quantities contaminated by error** (though such subtractions may be unavoidable)\n",
    "0. **Minimize the size of intermediate quantities relative to the finalsolution**\n",
    "    0. Otherwise the final answer may be the result of damaging subtractive cancellation\n",
    "    0. See Gaussian elimination or recursive summation\n",
    "0. **Look for different formulations of a computation that are mathematically but not numerically equivalent**\n",
    "    0. The classical Gram-Schmidt method is unstable, but a trivial modification produces the stable modified Gram-Schmidt (MGS) method\n",
    "    0. There are two ways of using the MGS method to solve a least squares problem, the more obvious of which is unstable.\n",
    "0. **It is advantageous to express update formulae as:**  \n",
    "***new_value = old_value + small_correction***  \n",
    "**if the small correction can be computed with many correct significant figures**  \n",
    "Numerical methods are often naturally expressed in this form, examples include\n",
    "    0. Methods for solving ordinary differential equations, where the correction is proportional to a stepsize\n",
    "    0. Newton's method for solving a nonlinear system\n",
    "    0. A classic example of the use of this update strategy is in iterative refinement for improving the computed solution to a linear system:  \n",
    "Ax = b  \n",
    "    in which by computing residuals r = b - Ay in extended precision and solving update equations that have the residuals as right-hand sides a highly accurate solution can be computed\n",
    "0. **Use only well-conditioned transformations of the problem**\n",
    "     0. In matrix computations this amounts to multiplying by orthogonal matrices instead of nonorthogonal, and possibly, ill-conditioned matrices, where possible\n",
    "0. **Take precautions to avoid unnecessary overflow and underflow**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
